diff --git a/__pycache__/ava.cpython-310.pyc b/__pycache__/ava.cpython-310.pyc
index 7a5be4e..24107c7 100644
Binary files a/__pycache__/ava.cpython-310.pyc and b/__pycache__/ava.cpython-310.pyc differ
diff --git a/__pycache__/config.cpython-310.pyc b/__pycache__/config.cpython-310.pyc
index 48cd0ba..65f3b24 100644
Binary files a/__pycache__/config.cpython-310.pyc and b/__pycache__/config.cpython-310.pyc differ
diff --git a/__pycache__/download.cpython-310.pyc b/__pycache__/download.cpython-310.pyc
index 99a52f7..d356cdc 100644
Binary files a/__pycache__/download.cpython-310.pyc and b/__pycache__/download.cpython-310.pyc differ
diff --git a/__pycache__/inference.cpython-310.pyc b/__pycache__/inference.cpython-310.pyc
index 8f47273..e6e65f4 100644
Binary files a/__pycache__/inference.cpython-310.pyc and b/__pycache__/inference.cpython-310.pyc differ
diff --git a/ava.py b/ava.py
index 43f5d3e..9104821 100644
--- a/ava.py
+++ b/ava.py
@@ -131,7 +131,7 @@ def make_class_dir(df, y_g_dict):
           ⌊_class 0
           ⌊_class 1'''
 
-    os.makedirs('data/', exist_ok=True)
+    os.makedirs('../data/', exist_ok=True)
     train_dir = '../data/train/'
     test_dir = '../data/test/'
     #!rm -rf data/train/ && rm -rf data/test/
@@ -146,14 +146,14 @@ def make_class_dir(df, y_g_dict):
         y_g_dict[key]['fid'] = '../data/test/'+im_id
         try:
 
-            os.symlink('../images/'+im_id, 'data/test/'+im_id)
+            os.symlink('../images/'+im_id, '../data/test/'+im_id)
         except:
             not_loaded.append(im_id)
     train_df = df[df['set'].isin(['training', 'validation'])]
     train_set = train_df['image_name'].values
     for im_id in tqdm(train_set, colour=('#FF69B4')):
         key = im_id.strip('.jpg')
-        y_g_dict[key]['fid'] = '../data/train/'+im_id
+        y_g_dict[key]['fid'] = '../images/'+im_id
         try:
 
             os.symlink('../images/'+im_id, '../data/train/'+im_id)
@@ -425,56 +425,55 @@ def plot_transform(data_dict):
     plt.savefig('transfroms.png', dpi=300)
 
 
-def data_samplers(reflect_transforms:dict,data:dict,ava_data_reflect:object, batch_size=None):
+def data_samplers(data, ava_data_reflect,batch_size=None):
     '''retrurns data loaders called during training'''
     test_ids = [idx for idx in data['training']][:20]
     # a small subset for debugging if needed <^_^> 
     data_tester = {key:data['training'][key] for key in test_ids}
-   
-    print(f' {"*"*30}ids = {test_ids} data = {data_tester}')
+    #change back
 
-    train_data_loader = ava_data_reflect(
+    train_data_loader =  ava_data_reflect(
         data['training'], transform=reflect_transforms['training']
-    )
-    val_data_loader = ava_data_reflect(
+        )
+    val_data_loader =  ava_data_reflect(
         data['validation'], transform=reflect_transforms['validation']
-    )
-    test_data_loader = ava_data_reflect(
+        )
+    test_data_loader =  ava_data_reflect(
         data['test'], transform=reflect_transforms['test']
-    )
+        )
     #Let there be 9 samples and 1 sample in class 0 and 1 respectively
     labels = [data['training'][idx]['threshold'] for idx in data['training']]
     class_counts = np.bincount(labels)
     num_samples = sum(class_counts)
     #corresponding labels of samples
-    class_weights = [num_samples/class_counts[i]
-                     for i in range(len(class_counts))]
+    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]
     weights = [class_weights[labels[i]] for i in range(int(num_samples))]
     sampler = torch.utils.data.WeightedRandomSampler(
         torch.DoubleTensor(weights), int(num_samples)
-    )
+        )
     print(len(weights))
     print(class_weights)
     sampler = torch.utils.data.WeightedRandomSampler(
         torch.DoubleTensor(weights), int(len(data['training'].keys()))
-    )
+        )
     # with data sampler (note ->> must be same len[-,...,-] as train set!!)
     train_loader = DataLoader(
-        dataset=train_data_loader,
-        sampler=sampler,
+        dataset = train_data_loader, 
+        sampler=sampler, 
         batch_size=batch_size,
-        shuffle=False, num_workers=4, pin_memory=True
-    )
+        shuffle=False
+        )
 
     val_loader = DataLoader(
-        dataset=val_data_loader,
-        batch_size=batch_size,
+        dataset = val_data_loader, 
+        batch_size=batch_size, 
         shuffle=False
-    )
+        )
     test_loader = DataLoader(
-        dataset=test_data_loader,
-        batch_size=batch_size, shuffle=True)
-    return {'training': train_loader, 'validation': val_loader, 'test': test_loader}
+        dataset = test_data_loader,
+         batch_size=batch_size, shuffle=True)
+    return {'training':train_loader,'validation':val_loader, 'test': test_loader}
+
 
 
 def seed_everything(seed):
@@ -681,7 +680,7 @@ def loader(models):
         yield model, models[mod]['epochs'], mod
 
 
-def deep_eval(model):
+def deep_eval(model,data_load_dict):
     '''validatioan loop ruturns metrics dict for passed model'''
     criterion = nn.CrossEntropyLoss()
     if torch.cuda.is_available():
diff --git a/config.py b/config.py
index c59f3da..39f36a5 100644
--- a/config.py
+++ b/config.py
@@ -32,6 +32,8 @@ parser.add_argument('--project', type=str, default=None,
                     help='project where runs saved')
 parser.add_argument('--tags', type=str, default=None,
                     help='run tags')
+parser.add_argument('--unzip_only', action='store_true',
+                    help='unzip_file')
 
 parser.add_argument('-d', type=Path, default='wandb/wandb/settings')
 args = parser.parse_args()
diff --git a/download.py b/download.py
index e7f4100..9da8fd6 100644
--- a/download.py
+++ b/download.py
@@ -44,7 +44,8 @@ class get_dataset:
         '''checks if images dir exists
         makes if not, then infaltes/decompreses to directory'''
         out, fid = Path(out_dir), Path(self.fid)
+        print(fid)
         if not out.is_dir():
             out.mkdir()
-        with zipfile.ZipExtFile(out, 'r') as hndl:
+        with zipfile.ZipFile(fid, 'r') as hndl:
             hndl.extractall(out)
diff --git a/inference.py b/inference.py
index a9d6e32..c79046c 100644
--- a/inference.py
+++ b/inference.py
@@ -432,10 +432,9 @@ def deep_eval(model,data_load_dict:dict, model_name=None):
         model.eval()
         for data, label, fid in tqdm(data_load_dict['test']):
             data = data.to(device)
-            for img in data:
-              images.append(wandb.Image(img))
-            for lab in label:
-              labels = np.append(labels, [lab])
+            images.append(wandb.Image(data))
+            print(label)
+            labels = np.append(labels, [label])
             label = label.to(device)
             t = time()
             output = model(data)
diff --git a/main.py b/main.py
index 050e64d..01eba8c 100644
--- a/main.py
+++ b/main.py
@@ -6,10 +6,11 @@ import gc
 import platform
 
 from config import args
-from ava import get_all, data_samplers, data_transforms, ava_data_reflect
+from ava import get_all, data_samplers, data_transforms, ava_data_reflect, deep_eval
 from download import get_dataset
 if args.inference:
-    from inference import deep_eval
+    print('evealuating model')
+    #from inference import deep_eval
 else:
     import ava
 
@@ -22,15 +23,21 @@ if __name__ == '__main__':
             fid='crushed.zip', url='http://desigley.space/ava/crushed.zip')
         get_data.get_zip()
         get_data.unzip(out_dir='images')
+    elif args.unzip_only:
+        get_data = get_dataset(
+            fid='../crushed.zip', url=None)
+        get_data.unzip(out_dir='../images')
+    
 
     df, y_g_dict, data, neg, pos = get_all(subset=True)
     reflect_transforms = data_transforms(size=224)
-    data_loader = data_samplers(
-        data=data, 
-        reflect_transforms=reflect_transforms,
-        ava_data_reflect=ava_data_reflect,
-        batch_size=args.batch_size)
+    # data_loader = data_samplers(
+    #     data=data, 
+    #     reflect_transforms=reflect_transforms,
+    #     ava_data_reflect=ava_data_reflect,
+    #     batch_size=args.batch_size)
     if args.inference:
+
         wandb.login()
         print(f'running in inference mode{"8"*30}')
         wb_tags = ['inference', platform.system(), platform.system(),
@@ -48,14 +55,17 @@ if __name__ == '__main__':
                         arg_= arg_.strip('\n').strip(' ')
                         param = param.strip('\n').strip(' ')
                         args.arg_=param
-            run = wandb.init(entity=args.entity, project=args.project)        
+            run = wandb.init(entity=args.entity, project=args.project)
+        else:
+            run = wandb.init()      
         model = timm.create_model('mobilevit_xxs')
         model.head.fc.out_features = 2
         loaded = torch.load('models/mobilevit_xxs',
                             map_location=torch.device(args.device))
         model.load_state_dict(loaded['model_state_dict'])
         run.watch(model)
-        evaluation = deep_eval(model, data_load_dict=data_loader)
+        data_load_dict = data_samplers(data,ava_data_reflect,batch_size=128)
+        evaluation = deep_eval(model, data_load_dict=data_load_dict)
         print('logging wandb table')
         run.finish()
     else:
